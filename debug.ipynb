{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0542310-52ea-4d7d-a6c7-4fd35481516c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/Bio/SubsMat/__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n",
      "/h/benjami/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core.init: Checking for fconfig files in pwd and ./rosetta/flags\n",
      "core.init: Rosetta version: PyRosetta4.conda.linux.cxx11thread.serialization.CentOS.python39.Release r337 2022.49+release.201d763 201d7639f91f369d58b1adf514f3febaf6154c58 http://www.pyrosetta.org 2022-12-07T16:15:33\n",
      "core.init: command: PyRosetta -ex1 -ex2aro -database /h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/pyrosetta/database\n",
      "basic.random.init_random_generator: 'RNG device' seed mode, using '/dev/urandom', seed=1495705403 seed_offset=0 real_seed=1495705403 thread_index=0\n",
      "basic.random.init_random_generator: RandomGenerator:init: Normal mode, seed=1495705403 RG_type=mt19937\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import Bio\n",
    "from Bio.Align import substitution_matrices\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from abnumber import Chain\n",
    "from xlm.utils import AttrDict\n",
    "from xlm.utils import bool_flag, initialize_exp\n",
    "from xlm.data.dictionary import Dictionary\n",
    "from xlm.model.transformer import TransformerModel\n",
    "from xlm.utils import to_cuda\n",
    "from xlm.model.transformer import get_masks\n",
    "from xlm.evaluation.evaluator import convert_to_text, calculate_identity\n",
    "from evaluation.datasets import SAbDabDataset\n",
    "from evaluation.datasets import get_dataset\n",
    "from evaluation.utils.protein.writers import save_pdb\n",
    "from evaluation.utils.data import *\n",
    "from evaluation.utils.misc import *\n",
    "from evaluation.utils.transforms import *\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "import pyrosetta\n",
    "pyrosetta.init(silent=True)\n",
    "\n",
    "from pyrosetta import pose_from_pdb, init\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cab81f-426f-4fc2-8dd7-3b08de9c5485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core.init: Rosetta version: PyRosetta4.conda.linux.cxx11thread.serialization.CentOS.python39.Release r337 2022.49+release.201d763 201d7639f91f369d58b1adf514f3febaf6154c58 http://www.pyrosetta.org 2022-12-07T16:15:33\n",
      "core.init: command: PyRosetta -use_input_sc -ignore_unrecognized_res -check_cdr_chainbreaks false -ignore_zero_occupancy false -load_PDB_components false -no_fconfig -database /h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/pyrosetta/database\n",
      "basic.random.init_random_generator: 'RNG device' seed mode, using '/dev/urandom', seed=1850212661 seed_offset=0 real_seed=1850212661 thread_index=0\n",
      "basic.random.init_random_generator: RandomGenerator:init: Normal mode, seed=1850212661 RG_type=mt19937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28161/2236608810.py:2: UserWarning: Import of 'rosetta' as a top-level module is deprecated and may be removed in 2018, import via 'pyrosetta.rosetta'.\n",
      "  from rosetta.core.select import residue_selector as selections\n"
     ]
    }
   ],
   "source": [
    "#Core Includes\n",
    "from rosetta.core.select import residue_selector as selections\n",
    "\n",
    "from rosetta.protocols import antibody\n",
    "init('-use_input_sc -ignore_unrecognized_res -check_cdr_chainbreaks false \\\n",
    "     -ignore_zero_occupancy false -load_PDB_components false -no_fconfig', silent=True)\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"\n",
    "    Generate a parameters parser.\n",
    "    \"\"\"\n",
    "    # parse parameters\n",
    "    parser = argparse.ArgumentParser(description=\"Translate sentences\")\n",
    "\n",
    "    # main parameters\n",
    "    parser.add_argument(\"--dump_path\", type=str, default=\"dumped/\", help=\"Experiment dump path\")\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"kir\", help=\"Experiment name\")\n",
    "    parser.add_argument(\"--exp_id\", type=str, default=\"123\", help=\"Experiment ID\")\n",
    "    parser.add_argument(\"--beam_size\", type=int, default=100)\n",
    "    parser.add_argument(\"--excess_res\", type=int, default=50)\n",
    "    parser.add_argument(\"--reporter\", type=bool, default=False)\n",
    "    # model / output paths\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"/scratch/ssd004/scratch/benjami/AntiXLM/evaluation/checkpoint_rand.pkl\", help=\"Model path\")\n",
    "    parser.add_argument(\"--output_path\", type=str, default=\"evaluation/\", help=\"Output path\")\n",
    "\n",
    "    # source language / target language\n",
    "    parser.add_argument(\"--src_lang\", type=str, default=\"ag\", help=\"Source language\")\n",
    "    parser.add_argument(\"--tgt_lang\", type=str, default=\"ab\", help=\"Target language\")\n",
    "\n",
    "    parser.add_argument('-i', '--index', type=int, default=0)\n",
    "    parser.add_argument('-t', '--tag', type=str, default='')\n",
    "    parser.add_argument('-c', '--config', type=str, default='evaluation/configs/test/codesign_single.yml')\n",
    "    parser.add_argument('-o', '--out_root', type=str, default='evaluation/results')\n",
    "\n",
    "    return parser\n",
    "\n",
    "class SabdabEntry:\n",
    "    def __init__(self, dataset, index, params, renumber=None) -> None:\n",
    "        self.structure = dataset[index]\n",
    "        self.entry = self.find_entry(dataset, index)\n",
    "        \n",
    "        self.structure_id = self.structure['id']\n",
    "        self.ag_name = self.entry['ag_name']\n",
    "        self.ag_chain = self.entry['ag_chains'][0]\n",
    "        self.ab_chain = self.entry['H_chain']\n",
    "        self.pdb_code = self.entry['pdbcode']\n",
    "\n",
    "        self.f1 = self.structure['heavy']['FW1_seq']\n",
    "        self.f2 = self.structure['heavy']['FW2_seq']\n",
    "        self.f3 = self.structure['heavy']['FW3_seq']\n",
    "        self.f4 = self.structure['heavy']['FW4_seq']\n",
    "\n",
    "        self.c1 = self.structure['heavy']['H1_seq']\n",
    "        self.c2 = self.structure['heavy']['H2_seq']\n",
    "        self.c3 = self.structure['heavy']['H3_seq']\n",
    "        \n",
    "        self.excess = params.excess_res\n",
    "\n",
    "        self.ab_seq = self.structure['heavy'].seq\n",
    "        self.ag_seq = self.structure['antigen'].seq\n",
    "        self.weights = {}\n",
    "        self.set_weights()\n",
    "        data_native = MergeChains()(self.structure)\n",
    "        self.log_dir = get_new_log_dir(os.path.join(params.log_dir), prefix='%02d_%s' % (index, self.structure_id))\n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'reference.pdb'))\n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'reference_renamed.pdb'), \n",
    "                 rename={self.ag_chain:'A', self.ab_chain:'H'})\n",
    "        pose = pose_from_pdb(os.path.join(self.log_dir, 'reference_renamed.pdb'))\n",
    "        ab_info = antibody.AntibodyInfo(pose, antibody.Chothia_Scheme, antibody.North)\n",
    "\n",
    "        for s in range(5,25):\n",
    "            self.epi_residues = np.array(antibody.select_epitope_residues(ab_info, pose, s))[len(self.ab_seq):]\n",
    "            if self.epi_residues.sum() > 20:\n",
    "                break\n",
    "        self.epi_range = (np.argmax(self.epi_residues), self.epi_residues.shape[0] - np.argmax(self.epi_residues[::-1]) - 1)\n",
    "        self.epi_resseq = self.structure['antigen']['resseq'][self.epi_residues]\n",
    "\n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'antigen.pdb'), ignore_chain=self.ab_chain)\n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'antibody.pdb'), ignore_chain=self.ag_chain)        \n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'cutted_antigen.pdb'), ignore_chain=self.ab_chain,\n",
    "                 write_range={self.ag_chain: (max(0, self.epi_range[0]-self.excess), self.epi_range[1]+self.excess)})        \n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'cutted_refrence_renamed.pdb'),\n",
    "                 write_range={'A': (max(0, self.epi_range[0]-self.excess), self.epi_range[1]+self.excess)},\n",
    "                 rename={self.ag_chain:'A', self.ab_chain:'H'})\n",
    "\n",
    "        self.identity = {}\n",
    "        self.generated_sequences = {}\n",
    "\n",
    "    @property\n",
    "    def antigen(self):\n",
    "        extra = int(min(self.excess, (240 - (self.epi_range[1] - self.epi_range[0]))/2))\n",
    "        return self.ag_seq[max(0, self.epi_range[0]-extra): self.epi_range[1]+extra]\n",
    "\n",
    "    @property\n",
    "    def antibody(self):\n",
    "        return self.ab_seq\n",
    "\n",
    "    def set_weights(self):\n",
    "        self.weights['CDR1'] = self._construct_weight(cdr1=True)\n",
    "        self.weights['CDR2'] = self._construct_weight(cdr2=True)\n",
    "        self.weights['CDR3'] = self._construct_weight(cdr3=True)\n",
    "        self.weights['CDR123'] = self._construct_weight(cdr1=True, cdr2=True, cdr3=True)\n",
    "\n",
    "    def _construct_weight(self, cdr1=False, cdr2=False, cdr3=False):\n",
    "        return [0] * len(self.f1) + \\\n",
    "            ([1] * len(self.c1) if cdr1 else [0] * len(self.c1)) + \\\n",
    "            [0] * len(self.f2) + \\\n",
    "            ([1] * len(self.c2) if cdr2 else [0] * len(self.c2)) + \\\n",
    "            [0] * len(self.f3) + \\\n",
    "            ([1] * len(self.c3) if cdr3 else [0] * len(self.c3)) + \\\n",
    "            [0] * len(self.f4)\n",
    "\n",
    "\n",
    "    def find_entry(self, dataset:SAbDabDataset, index):\n",
    "        for entry in dataset.sabdab_entries:\n",
    "            if entry['id'] == self.structure['id']:\n",
    "                return entry\n",
    "\n",
    "    def write_generated(self):\n",
    "        for key in self.generated_sequences:\n",
    "        # Create a file name for the fasta file\n",
    "            with open(os.path.join(self.log_dir, key)+'.fasta', \"w\") as f:\n",
    "                for i, seq in enumerate(self.generated_sequences[key]):\n",
    "                    # Open the file for writing\n",
    "                    f.write(\">{0}_sequence\".format(key) + str(i) + \"\\n\")\n",
    "                    # Write the sequence to the file in fasta format\n",
    "                    f.write(seq.replace(' ', '') + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3ac9fe-642a-45ef-965e-69b1e73ec4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "def get_model(params):    \n",
    "    # initialize the experiment\n",
    "    logger = initialize_exp(params)\n",
    "\n",
    "    # generate parser / parse parameters\n",
    "    reloaded = torch.load(params.model_path)\n",
    "    model_params = AttrDict(reloaded['params'])\n",
    "    logger.info(\"Supported languages: %s\" % \", \".join(model_params.lang2id.keys()))\n",
    "\n",
    "    # update dictionary parameters\n",
    "    for name in ['n_words', 'bos_index', 'eos_index', 'pad_index', 'unk_index', 'mask_index']:\n",
    "        setattr(params, name, getattr(model_params, name))\n",
    "\n",
    "    # build dictionary / build encoder / build decoder / reload weights\n",
    "    dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "    encoder = TransformerModel(model_params, dico, is_encoder=True, with_output=False).cuda().eval()\n",
    "    decoder = TransformerModel(model_params, dico, is_encoder=False, with_output=True).cuda().eval()\n",
    "    encoder.load_state_dict({k[7:]:reloaded['encoder'][k] for k in reloaded['encoder']})\n",
    "    decoder.load_state_dict({k[7:]:reloaded['decoder'][k] for k in reloaded['decoder']})\n",
    "    params.src_id = model_params.lang2id['ag']\n",
    "    params.tgt_id = model_params.lang2id['ab']\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    bert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    bert.eval()\n",
    "    bert = bert.cuda()\n",
    "    return encoder, decoder, dico, bert\n",
    "\n",
    "\n",
    "def get_sabdab(params):\n",
    "    # Load configs\n",
    "    config, config_name = load_config(params.config)\n",
    "    # Testset\n",
    "    dataset = get_dataset(config.dataset.test)\n",
    "    # Logging\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_batch(seq, lang, eos, bos, pad):\n",
    "    lengths = torch.LongTensor([len(seq) + 2])\n",
    "    batch = torch.LongTensor(lengths.max().item(), lengths.size(0)).fill_(pad)\n",
    "    batch[0] = bos\n",
    "    batch[1:lengths[0] - 1, 0].copy_(seq)\n",
    "    batch[lengths[0] - 1, 0] = eos\n",
    "    langs = batch.clone().fill_(lang)\n",
    "    return batch, lengths, langs\n",
    "\n",
    "\n",
    "def write_generated(log_dir, eval_step, sequences):\n",
    "    log = get_new_log_dir(os.path.join(log_dir, eval_step))\n",
    "    for i, seq in enumerate(sequences):\n",
    "    # Create a file name for the fasta file\n",
    "        filename = \"sequence\" + str(i) + \".fasta\"\n",
    "\n",
    "        # Open the file for writing\n",
    "        with open(os.path.join(log, filename), \"w\") as f:\n",
    "            # Write the sequence to the file in fasta format\n",
    "            f.write(\">sequence\" + str(i) + \"\\n\")\n",
    "            f.write(seq.replace(' ', '') + \"\\n\")\n",
    "\n",
    "def evaluate(encoder, decoder, dico, bert, params, aligner, sample:SabdabEntry, eval_modes=['CDR1', 'CDR2', 'CDR3', 'CDR123', 'GEN']):\n",
    "    \n",
    "    ag_tensor, ag_length_tensor, ag_langs_tensor = build_batch(torch.LongTensor([dico.index(w) for w in sample.antigen]), \n",
    "                                                               params.tgt_id, params.eos_index, params.bos_index, params.pad_index)\n",
    "    ab_tensor, ab_length_tensor, ab_langs_tensor = build_batch(torch.LongTensor([dico.index(w) for w in sample.antibody]), \n",
    "                                                               params.tgt_id, params.eos_index, params.bos_index, params.pad_index)\n",
    "    token_type_ids = torch.zeros_like(ag_tensor)\n",
    "            \n",
    "    ab_tensor, ab_length_tensor, ab_langs_tensor, ag_tensor, ag_length_tensor, ag_langs_tensor, token_type_ids = \\\n",
    "        to_cuda(ab_tensor, ab_length_tensor, ab_langs_tensor, ag_tensor, ag_length_tensor, ag_langs_tensor, token_type_ids)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):   \n",
    "        bert_embed = bert(input_ids=ag_tensor.T, token_type_ids=token_type_ids.T, attention_mask=get_masks(ag_tensor.size()[0], ag_length_tensor, False)[0]).last_hidden_state\n",
    "\n",
    "        enc1 = encoder('fwd', x=ag_tensor, lengths=ag_length_tensor, langs=ag_langs_tensor, causal=False, bert_embed=bert_embed)\n",
    "        enc1 = enc1.transpose(0, 1)\n",
    "    beam_size = params.beam_size\n",
    "    for eval_step in eval_modes:\n",
    "        if eval_step == 'GEN':\n",
    "            beam_size = 20\n",
    "        ab_weights_tensor, _, _ = build_batch(torch.LongTensor(sample.weights.get(eval_step, sample.weights['CDR123'])), params.tgt_id, 1, 0, 0)\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):   \n",
    "            generated, lengths = decoder.generate_beam(\n",
    "                                enc1, ag_length_tensor, params.tgt_id, beam_size=beam_size,\n",
    "                                length_penalty=1,\n",
    "                                early_stopping=False,\n",
    "                                max_len=160,\n",
    "                                bert_embed=bert_embed,\n",
    "                                cdr_generation=eval_step != 'GEN',\n",
    "                                tgt_frw=ab_tensor,\n",
    "                                w=ab_weights_tensor,\n",
    "                                open_end=False\n",
    "                            )\n",
    "            hypothesis_text = convert_to_text(generated, lengths, dico, params)\n",
    "            batch_generate_identity, batch_generate_cdr_identity = calculate_identity(aligner, [sample.antibody], hypothesis_text, 'ab', ab_weights_tensor, beam_size=beam_size)\n",
    "            write_generated(sample.log_dir, eval_step, hypothesis_text)\n",
    "            sample.generated_sequences[eval_step] = hypothesis_text\n",
    "            \n",
    "            if eval_step == 'GEN':\n",
    "                sample.identity[eval_step+'_CDR'] = batch_generate_cdr_identity\n",
    "                sample.identity[eval_step+'_ALL'] = batch_generate_identity\n",
    "            else:\n",
    "                sample.identity[eval_step] = batch_generate_cdr_identity\n",
    "    print(sample.identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e878a2-b1cd-4102-88ee-b426188a9ace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 04/12/23 14:51:31 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 04/12/23 14:51:31 - 0:00:00 - beam_size: 100\n",
      "                                     command: python /h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/ipykernel_launcher.py '-f' '/ssd003/home/benjami/.local/share/jupyter/runtime/kernel-32dceed6-a15c-4c04-9b75-8cbd78f35e43.json' --exp_id \"123\"\n",
      "                                     config: evaluation/configs/test/codesign_single.yml\n",
      "                                     dump_path: dumped/kir/123\n",
      "                                     excess_res: 50\n",
      "                                     exp_id: 123\n",
      "                                     exp_name: kir\n",
      "                                     index: 0\n",
      "                                     log_dir: evaluation/results/2023_04_12__14_51_30\n",
      "                                     model_path: /scratch/ssd004/scratch/benjami/AntiXLM/evaluation/checkpoint_rand.pkl\n",
      "                                     out_root: evaluation/results\n",
      "                                     output_path: evaluation/\n",
      "                                     reporter: False\n",
      "                                     src_lang: ag\n",
      "                                     tag: \n",
      "                                     tgt_lang: ab\n",
      "INFO - 04/12/23 14:51:31 - 0:00:00 - The experiment will be stored in dumped/kir/123\n",
      "                                     \n",
      "INFO - 04/12/23 14:51:31 - 0:00:00 - Running command: python /h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/ipykernel_launcher.py '-f' '/ssd003/home/benjami/.local/share/jupyter/runtime/kernel-32dceed6-a15c-4c04-9b75-8cbd78f35e43.json'\n",
      "\n",
      "INFO - 04/12/23 14:51:35 - 0:00:04 - Supported languages: ab, ag\n",
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "parser = get_parser()\n",
    "params = parser.parse_args(args=[])\n",
    "log = get_new_log_dir(os.path.join(params.out_root), date=True)\n",
    "params.log_dir = log\n",
    "dataset = get_sabdab(params)\n",
    "test_samples = []\n",
    "\n",
    "encoder, decoder, dico, bert = get_model(params)\n",
    "# aligner = Bio.Align.PairwiseAligner()\n",
    "# aligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\n",
    "# aligner.open_gap_score = -10\n",
    "# aligner.extend_gap_score = -0.5    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96308db9-b621-4c2d-b0ec-521dc038b126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core.chemical.GlobalResidueTypeSet: Finished initializing fa_standard residue type set.  Created 985 residue types\n",
      "core.chemical.GlobalResidueTypeSet: Total time to initialize 2.96259 seconds.\n",
      "core.import_pose.import_pose: File 'evaluation/results/2023_04_12__14_51_30/00_4ffv_H_L_A_/reference_renamed.pdb' automatically determined to be of type PDB\n",
      "core.conformation.Conformation: [ WARNING ] missing heavyatom:  OXT on residue SER:CtermProteinFull 727\n",
      "core.conformation.Conformation: [ WARNING ] missing heavyatom:  OXT on residue SER:CtermProteinFull 844\n",
      "core.conformation.Conformation: Found disulfide between residues 288 299\n",
      "core.conformation.Conformation: current variant for 288 CYS\n",
      "core.conformation.Conformation: current variant for 299 CYS\n",
      "core.conformation.Conformation: current variant for 288 CYD\n",
      "core.conformation.Conformation: current variant for 299 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 345 357\n",
      "core.conformation.Conformation: current variant for 345 CYS\n",
      "core.conformation.Conformation: current variant for 357 CYS\n",
      "core.conformation.Conformation: current variant for 345 CYD\n",
      "core.conformation.Conformation: current variant for 357 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 407 410\n",
      "core.conformation.Conformation: current variant for 407 CYS\n",
      "core.conformation.Conformation: current variant for 410 CYS\n",
      "core.conformation.Conformation: current variant for 407 CYD\n",
      "core.conformation.Conformation: current variant for 410 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 417 435\n",
      "core.conformation.Conformation: current variant for 417 CYS\n",
      "core.conformation.Conformation: current variant for 435 CYS\n",
      "core.conformation.Conformation: current variant for 417 CYD\n",
      "core.conformation.Conformation: current variant for 435 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 612 725\n",
      "core.conformation.Conformation: current variant for 612 CYS\n",
      "core.conformation.Conformation: current variant for 725 CYS\n",
      "core.conformation.Conformation: current variant for 612 CYD\n",
      "core.conformation.Conformation: current variant for 725 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 749 823\n",
      "core.conformation.Conformation: current variant for 749 CYS\n",
      "core.conformation.Conformation: current variant for 823 CYS\n",
      "core.conformation.Conformation: current variant for 749 CYD\n",
      "core.conformation.Conformation: current variant for 823 CYD\n",
      "basic.io.database: Database file opened: sampling/antibodies/cluster_center_dihedrals.txt\n",
      "protocols.antibody.AntibodyNumberingParser: Antibody numbering scheme definitions read successfully\n",
      "protocols.antibody.AntibodyNumberingParser: Antibody CDR definition read successfully\n",
      "antibody.AntibodyInfo: Successfully finished the CDR definition\n",
      "antibody.AntibodyInfo:  Could not setup Vl Vh Packing angle for camelid antibody\n",
      "antibody.AntibodyInfo: AC Detecting Camelid CDR H3 Stem Type\n",
      "antibody.AntibodyInfo: AC Finished Detecting Camelid CDR H3 Stem Type: NEUTRAL\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H1\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 13 Omega: TTTTTTTTTTTTT\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H2\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 10 Omega: TTTTTTTTTT\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H3\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 10 Omega: TTTTTTTTTT\n",
      "{'CDR1': tensor(0.7857), 'CDR2': tensor(0.7617), 'CDR3': tensor(0.6775)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = SabdabEntry(dataset=dataset, index=0, params=params)\n",
    "\n",
    "evaluate(encoder, decoder, dico, bert, params, None, sample, eval_modes=['CDR1', 'CDR2', 'CDR3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8aa444-1d20-4af0-ae0b-30b7cb6f2e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core.import_pose.import_pose: File 'evaluation/results/2023_04_12__14_50_39/00_4ffv_H_L_A_/reference_renamed.pdb' automatically determined to be of type PDB\n",
      "core.conformation.Conformation: [ WARNING ] missing heavyatom:  OXT on residue SER:CtermProteinFull 117\n",
      "core.conformation.Conformation: [ WARNING ] missing heavyatom:  OXT on residue SER:CtermProteinFull 844\n",
      "core.conformation.Conformation: Found disulfide between residues 22 96\n",
      "core.conformation.Conformation: current variant for 22 CYS\n",
      "core.conformation.Conformation: current variant for 96 CYS\n",
      "core.conformation.Conformation: current variant for 22 CYD\n",
      "core.conformation.Conformation: current variant for 96 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 405 416\n",
      "core.conformation.Conformation: current variant for 405 CYS\n",
      "core.conformation.Conformation: current variant for 416 CYS\n",
      "core.conformation.Conformation: current variant for 405 CYD\n",
      "core.conformation.Conformation: current variant for 416 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 462 474\n",
      "core.conformation.Conformation: current variant for 462 CYS\n",
      "core.conformation.Conformation: current variant for 474 CYS\n",
      "core.conformation.Conformation: current variant for 462 CYD\n",
      "core.conformation.Conformation: current variant for 474 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 524 527\n",
      "core.conformation.Conformation: current variant for 524 CYS\n",
      "core.conformation.Conformation: current variant for 527 CYS\n",
      "core.conformation.Conformation: current variant for 524 CYD\n",
      "core.conformation.Conformation: current variant for 527 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 534 552\n",
      "core.conformation.Conformation: current variant for 534 CYS\n",
      "core.conformation.Conformation: current variant for 552 CYS\n",
      "core.conformation.Conformation: current variant for 534 CYD\n",
      "core.conformation.Conformation: current variant for 552 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 729 842\n",
      "core.conformation.Conformation: current variant for 729 CYS\n",
      "core.conformation.Conformation: current variant for 842 CYS\n",
      "core.conformation.Conformation: current variant for 729 CYD\n",
      "core.conformation.Conformation: current variant for 842 CYD\n",
      "basic.io.database: Database file opened: sampling/antibodies/cluster_center_dihedrals.txt\n",
      "protocols.antibody.AntibodyNumberingParser: Antibody numbering scheme definitions read successfully\n",
      "protocols.antibody.AntibodyNumberingParser: Antibody CDR definition read successfully\n",
      "antibody.AntibodyInfo: Successfully finished the CDR definition\n",
      "antibody.AntibodyInfo:  Could not setup Vl Vh Packing angle for camelid antibody\n",
      "antibody.AntibodyInfo: AC Detecting Camelid CDR H3 Stem Type\n",
      "antibody.AntibodyInfo: AC Finished Detecting Camelid CDR H3 Stem Type: NEUTRAL\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H1\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 13 Omega: TTTTTTTTTTTTT\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H2\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 10 Omega: TTTTTTTTTT\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H3\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 10 Omega: TTTTTTTTTT\n"
     ]
    }
   ],
   "source": [
    "sample = SabdabEntry(dataset=dataset, index=0, params=params)\n",
    "\n",
    "evaluate(encoder, decoder, dico, bert, params, None, sample, eval_modes=['CDR1', 'CDR2', 'CDR3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae60f48-b172-4aac-86bb-3773d1f93484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('evaluation/results/2023_04_12__14_49_18/00_4ffv_H_L_A_/sample.pkl', 'rb') as f:\n",
    "    sample2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e16721ed-6636-4bc5-9319-3546362cc09e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.generated_sequences['CDR3'][10] == sample.generated_sequences['CDR3'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b57ea9d-5dcd-4237-b0d4-4e93412160b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2.generated_sequences['CDR3'][0] == sample2.generated_sequences['CDR3'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2077e-e027-4a45-99e6-9f40bc5f52e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AntiXLM",
   "language": "python",
   "name": "antixlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
