{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53eb4b80-18bd-4f07-9cad-50d95cc089a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/h/benjami/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/Bio/SubsMat/__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core.init: Checking for fconfig files in pwd and ./rosetta/flags\n",
      "core.init: Rosetta version: PyRosetta4.conda.linux.cxx11thread.serialization.CentOS.python39.Release r337 2022.49+release.201d763 201d7639f91f369d58b1adf514f3febaf6154c58 http://www.pyrosetta.org 2022-12-07T16:15:33\n",
      "core.init: command: PyRosetta -ex1 -ex2aro -database /h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/pyrosetta/database\n",
      "basic.random.init_random_generator: 'RNG device' seed mode, using '/dev/urandom', seed=964659220 seed_offset=0 real_seed=964659220 thread_index=0\n",
      "basic.random.init_random_generator: RandomGenerator:init: Normal mode, seed=964659220 RG_type=mt19937\n",
      "core.init: Rosetta version: PyRosetta4.conda.linux.cxx11thread.serialization.CentOS.python39.Release r337 2022.49+release.201d763 201d7639f91f369d58b1adf514f3febaf6154c58 http://www.pyrosetta.org 2022-12-07T16:15:33\n",
      "core.init: command: PyRosetta -use_input_sc -ignore_unrecognized_res -check_cdr_chainbreaks false -ignore_zero_occupancy false -load_PDB_components false -no_fconfig -database /h/benjami/.conda/envs/AntiXLM/lib/python3.9/site-packages/pyrosetta/database\n",
      "basic.random.init_random_generator: 'RNG device' seed mode, using '/dev/urandom', seed=1507664767 seed_offset=0 real_seed=1507664767 thread_index=0\n",
      "basic.random.init_random_generator: RandomGenerator:init: Normal mode, seed=1507664767 RG_type=mt19937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34855/111924882.py:39: UserWarning: Import of 'rosetta' as a top-level module is deprecated and may be removed in 2018, import via 'pyrosetta.rosetta'.\n",
      "  from rosetta.core.select import residue_selector as selections\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import Bio\n",
    "from Bio.Align import substitution_matrices\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from abnumber import Chain\n",
    "from evaluation.datasets import SAbDabDataset\n",
    "from evaluation.datasets import get_dataset\n",
    "from evaluation.utils.protein.writers import save_pdb\n",
    "from evaluation.utils.data import *\n",
    "from evaluation.utils.misc import *\n",
    "from evaluation.utils.transforms import *\n",
    "from xlm.utils import AttrDict\n",
    "from xlm.utils import bool_flag, initialize_exp\n",
    "from xlm.data.dictionary import Dictionary\n",
    "from xlm.model.transformer import TransformerModel\n",
    "from xlm.utils import to_cuda\n",
    "from xlm.model.transformer import get_masks\n",
    "from xlm.evaluation.evaluator import convert_to_text, calculate_identity\n",
    "from transformers import BertModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pyrosetta\n",
    "pyrosetta.init(silent=True)\n",
    "\n",
    "from pyrosetta import pose_from_pdb, init\n",
    "# from pyrosetta.rosetta import *\n",
    "# from pyrosetta.teaching import *\n",
    "\n",
    "#Core Includes\n",
    "from rosetta.core.select import residue_selector as selections\n",
    "\n",
    "from rosetta.protocols import antibody\n",
    "init('-use_input_sc -ignore_unrecognized_res -check_cdr_chainbreaks false \\\n",
    "     -ignore_zero_occupancy false -load_PDB_components false -no_fconfig', silent=True)\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "    \"\"\"\n",
    "    Generate a parameters parser.\n",
    "    \"\"\"\n",
    "    # parse parameters\n",
    "    parser = argparse.ArgumentParser(description=\"Translate sentences\")\n",
    "\n",
    "    # main parameters\n",
    "    parser.add_argument(\"--dump_path\", type=str, default=\"dumped/\", help=\"Experiment dump path\")\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"kir\", help=\"Experiment name\")\n",
    "    parser.add_argument(\"--exp_id\", type=str, default=\"123\", help=\"Experiment ID\")\n",
    "    parser.add_argument(\"--beam_size\", type=int, default=100)\n",
    "    parser.add_argument(\"--excess_res\", type=int, default=50)\n",
    "    parser.add_argument(\"--reporter\", type=bool, default=False)\n",
    "    # model / output paths\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"/scratch/ssd004/scratch/benjami/AntiXLM/evaluation/checkpoint_rand.pkl\", help=\"Model path\")\n",
    "    parser.add_argument(\"--output_path\", type=str, default=\"evaluation/\", help=\"Output path\")\n",
    "\n",
    "    # source language / target language\n",
    "    parser.add_argument(\"--src_lang\", type=str, default=\"ag\", help=\"Source language\")\n",
    "    parser.add_argument(\"--tgt_lang\", type=str, default=\"ab\", help=\"Target language\")\n",
    "\n",
    "    parser.add_argument('-i', '--index', type=int, default=0)\n",
    "    parser.add_argument('-t', '--tag', type=str, default='')\n",
    "    parser.add_argument('-c', '--config', type=str, default='evaluation/configs/test/codesign_single.yml')\n",
    "    parser.add_argument('-o', '--out_root', type=str, default='evaluation/results')\n",
    "\n",
    "    return parser\n",
    "\n",
    "class SabdabEntry:\n",
    "    def __init__(self, dataset, index, params, renumber=None) -> None:\n",
    "        self.structure = dataset[index]\n",
    "        self.entry = self.find_entry(dataset, index)\n",
    "        \n",
    "        self.structure_id = self.structure['id']\n",
    "        self.ag_name = self.entry['ag_name']\n",
    "        self.ag_chain = self.entry['ag_chains'][0]\n",
    "        self.ab_chain = self.entry['H_chain']\n",
    "        self.pdb_code = self.entry['pdbcode']\n",
    "\n",
    "        self.f1 = self.structure['heavy']['FW1_seq']\n",
    "        self.f2 = self.structure['heavy']['FW2_seq']\n",
    "        self.f3 = self.structure['heavy']['FW3_seq']\n",
    "        self.f4 = self.structure['heavy']['FW4_seq']\n",
    "\n",
    "        self.c1 = self.structure['heavy']['H1_seq']\n",
    "        self.c2 = self.structure['heavy']['H2_seq']\n",
    "        self.c3 = self.structure['heavy']['H3_seq']\n",
    "        \n",
    "        self.excess = params.excess_res\n",
    "\n",
    "        self.ab_seq = self.structure['heavy'].seq\n",
    "        self.ag_seq = self.structure['antigen'].seq\n",
    "        self.weights = {}\n",
    "        self.set_weights()\n",
    "        data_native = MergeChains()(self.structure)\n",
    "        self.log_dir = get_new_log_dir(os.path.join(params.log_dir), prefix='%02d_%s' % (index, self.structure_id))\n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'reference.pdb'))\n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'reference_renamed.pdb'), \n",
    "                 rename={self.ag_chain:'A', self.ab_chain:'H'})\n",
    "        pose = pose_from_pdb(os.path.join(self.log_dir, 'reference_renamed.pdb'))\n",
    "        ab_info = antibody.AntibodyInfo(pose, antibody.Chothia_Scheme, antibody.North)\n",
    "\n",
    "        for s in range(5,25):\n",
    "            self.epi_residues = np.array(antibody.select_epitope_residues(ab_info, pose, s))[len(self.ab_seq):]\n",
    "            if self.epi_residues.sum() > 20:\n",
    "                break\n",
    "        self.epi_range = (np.argmax(self.epi_residues), self.epi_residues.shape[0] - np.argmax(self.epi_residues[::-1]) - 1)\n",
    "        self.epi_resseq = self.structure['antigen']['resseq'][self.epi_residues]\n",
    "\n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'antigen.pdb'), ignore_chain=self.ab_chain)\n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'antibody.pdb'), ignore_chain=self.ag_chain)        \n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'cutted_antigen.pdb'), ignore_chain=self.ab_chain,\n",
    "                 write_range={self.ag_chain: (max(0, self.epi_range[0]-self.excess), self.epi_range[1]+self.excess)})        \n",
    "        save_pdb(data_native, os.path.join(self.log_dir, 'cutted_refrence_renamed.pdb'),\n",
    "                 write_range={'A': (max(0, self.epi_range[0]-self.excess), self.epi_range[1]+self.excess)},\n",
    "                 rename={self.ag_chain:'A', self.ab_chain:'H'})\n",
    "\n",
    "        self.identity = {}\n",
    "        self.generated_sequences = {}\n",
    "\n",
    "    @property\n",
    "    def antigen(self):\n",
    "        extra = int(min(self.excess, (240 - (self.epi_range[1] - self.epi_range[0]))/2))\n",
    "        return self.ag_seq[max(0, self.epi_range[0]-extra): self.epi_range[1]+extra]\n",
    "\n",
    "    @property\n",
    "    def antibody(self):\n",
    "        return self.ab_seq\n",
    "\n",
    "    def set_weights(self):\n",
    "        self.weights['CDR1'] = self._construct_weight(cdr1=True)\n",
    "        self.weights['CDR2'] = self._construct_weight(cdr2=True)\n",
    "        self.weights['CDR3'] = self._construct_weight(cdr3=True)\n",
    "        self.weights['CDR123'] = self._construct_weight(cdr1=True, cdr2=True, cdr3=True)\n",
    "\n",
    "    def _construct_weight(self, cdr1=False, cdr2=False, cdr3=False):\n",
    "        return [0] * len(self.f1) + \\\n",
    "            ([1] * len(self.c1) if cdr1 else [0] * len(self.c1)) + \\\n",
    "            [0] * len(self.f2) + \\\n",
    "            ([1] * len(self.c2) if cdr2 else [0] * len(self.c2)) + \\\n",
    "            [0] * len(self.f3) + \\\n",
    "            ([1] * len(self.c3) if cdr3 else [0] * len(self.c3)) + \\\n",
    "            [0] * len(self.f4)\n",
    "\n",
    "\n",
    "    def find_entry(self, dataset:SAbDabDataset, index):\n",
    "        for entry in dataset.sabdab_entries:\n",
    "            if entry['id'] == self.structure['id']:\n",
    "                return entry\n",
    "\n",
    "    def write_generated(self):\n",
    "        for key in self.generated_sequences:\n",
    "        # Create a file name for the fasta file\n",
    "            with open(os.path.join(self.log_dir, key)+'.fasta', \"w\") as f:\n",
    "                for i, seq in enumerate(self.generated_sequences[key]):\n",
    "                    # Open the file for writing\n",
    "                    f.write(\">{0}_sequence\".format(key) + str(i) + \"\\n\")\n",
    "                    # Write the sequence to the file in fasta format\n",
    "                    f.write(seq.replace(' ', '') + \"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "def get_model(params):    \n",
    "    # initialize the experiment\n",
    "    # logger = initialize_exp(params)\n",
    "\n",
    "    # generate parser / parse parameters\n",
    "    reloaded = torch.load(params.model_path)\n",
    "    model_params = AttrDict(reloaded['params'])\n",
    "    # logger.info(\"Supported languages: %s\" % \", \".join(model_params.lang2id.keys()))\n",
    "\n",
    "    # update dictionary parameters\n",
    "    for name in ['n_words', 'bos_index', 'eos_index', 'pad_index', 'unk_index', 'mask_index']:\n",
    "        setattr(params, name, getattr(model_params, name))\n",
    "\n",
    "    # build dictionary / build encoder / build decoder / reload weights\n",
    "    dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "    encoder = TransformerModel(model_params, dico, is_encoder=True, with_output=False).cuda().eval()\n",
    "    decoder = TransformerModel(model_params, dico, is_encoder=False, with_output=True).cuda().eval()\n",
    "    encoder.load_state_dict({k[7:]:reloaded['encoder'][k] for k in reloaded['encoder']})\n",
    "    decoder.load_state_dict({k[7:]:reloaded['decoder'][k] for k in reloaded['decoder']})\n",
    "    params.src_id = model_params.lang2id['ag']\n",
    "    params.tgt_id = model_params.lang2id['ab']\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    bert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "    bert.eval()\n",
    "    bert = bert.cuda()\n",
    "    return encoder, decoder, dico, bert\n",
    "\n",
    "\n",
    "def get_sabdab(params):\n",
    "    # Load configs\n",
    "    config, config_name = load_config(params.config)\n",
    "    # Testset\n",
    "    dataset = get_dataset(config.dataset.test)\n",
    "    # Logging\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_batch(seq, lang, eos, bos, pad):\n",
    "    lengths = torch.LongTensor([len(seq) + 2])\n",
    "    batch = torch.LongTensor(lengths.max().item(), lengths.size(0)).fill_(pad)\n",
    "    batch[0] = bos\n",
    "    batch[1:lengths[0] - 1, 0].copy_(seq)\n",
    "    batch[lengths[0] - 1, 0] = eos\n",
    "    langs = batch.clone().fill_(lang)\n",
    "    return batch, lengths, langs\n",
    "\n",
    "\n",
    "def write_generated(log_dir, eval_step, sequences):\n",
    "    log = get_new_log_dir(os.path.join(log_dir, eval_step))\n",
    "    for i, seq in enumerate(sequences):\n",
    "    # Create a file name for the fasta file\n",
    "        filename = \"sequence\" + str(i) + \".fasta\"\n",
    "\n",
    "        # Open the file for writing\n",
    "        with open(os.path.join(log, filename), \"w\") as f:\n",
    "            # Write the sequence to the file in fasta format\n",
    "            f.write(\">sequence\" + str(i) + \"\\n\")\n",
    "            f.write(seq.replace(' ', '') + \"\\n\")\n",
    "\n",
    "def evaluate(encoder, decoder, dico, bert, params, aligner, sample:SabdabEntry, eval_modes=['CDR1', 'CDR2', 'CDR3', 'CDR123', 'GEN']):\n",
    "    \n",
    "    ag_tensor, ag_length_tensor, ag_langs_tensor = build_batch(torch.LongTensor([dico.index(w) for w in sample.antigen]), \n",
    "                                                               params.tgt_id, params.eos_index, params.bos_index, params.pad_index)\n",
    "    ab_tensor, ab_length_tensor, ab_langs_tensor = build_batch(torch.LongTensor([dico.index(w) for w in sample.antibody]), \n",
    "                                                               params.tgt_id, params.eos_index, params.bos_index, params.pad_index)\n",
    "    token_type_ids = torch.zeros_like(ag_tensor)\n",
    "            \n",
    "    ab_tensor, ab_length_tensor, ab_langs_tensor, ag_tensor, ag_length_tensor, ag_langs_tensor, token_type_ids = \\\n",
    "        to_cuda(ab_tensor, ab_length_tensor, ab_langs_tensor, ag_tensor, ag_length_tensor, ag_langs_tensor, token_type_ids)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):  \n",
    "        with torch.no_grad(): \n",
    "            bert_embed = bert(input_ids=ag_tensor.T, token_type_ids=token_type_ids.T, attention_mask=get_masks(ag_tensor.size()[0], ag_length_tensor, False)[0]).last_hidden_state\n",
    "\n",
    "            enc1 = encoder('fwd', x=ag_tensor, lengths=ag_length_tensor, langs=ag_langs_tensor, causal=False, bert_embed=bert_embed)\n",
    "            enc1 = enc1.transpose(0, 1)\n",
    "    beam_size = params.beam_size\n",
    "    for eval_step in eval_modes:\n",
    "        if eval_step == 'GEN':\n",
    "            beam_size = 20\n",
    "        ab_weights_tensor, _, _ = build_batch(torch.LongTensor(sample.weights.get(eval_step, sample.weights['CDR123'])), params.tgt_id, 0, 0, 0)\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):   \n",
    "            with torch.no_grad(): \n",
    "                generated, lengths = decoder.generate_beam(\n",
    "                                    enc1, ag_length_tensor, params.tgt_id, beam_size=beam_size,\n",
    "                                    length_penalty=1,\n",
    "                                    early_stopping=False,\n",
    "                                    max_len=160,\n",
    "                                    bert_embed=bert_embed,\n",
    "                                    cdr_generation=eval_step != 'GEN',\n",
    "                                    tgt_frw=ab_tensor,\n",
    "                                    w=ab_weights_tensor,\n",
    "                                    open_end=False\n",
    "                                )\n",
    "        hypothesis_text = convert_to_text(generated, lengths, dico, params)\n",
    "        batch_generate_identity, batch_generate_cdr_identity = calculate_identity(aligner, [sample.antibody], hypothesis_text, 'ab', ab_weights_tensor, beam_size=beam_size)\n",
    "        write_generated(sample.log_dir, eval_step, hypothesis_text)\n",
    "        sample.generated_sequences[eval_step] = hypothesis_text\n",
    "        \n",
    "        if eval_step == 'GEN':\n",
    "            sample.identity[eval_step+'_CDR'] = batch_generate_cdr_identity\n",
    "            sample.identity[eval_step+'_ALL'] = batch_generate_identity\n",
    "        else:\n",
    "            sample.identity[eval_step] = batch_generate_cdr_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89382f03-9d5b-4322-a470-4447831300d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core.chemical.GlobalResidueTypeSet: Finished initializing fa_standard residue type set.  Created 985 residue types\n",
      "core.chemical.GlobalResidueTypeSet: Total time to initialize 3.14974 seconds.\n",
      "core.import_pose.import_pose: File 'evaluation/results/2023_04_30__16_50_20/00_4ffv_H_L_A_/reference_renamed.pdb' automatically determined to be of type PDB\n",
      "core.conformation.Conformation: [ WARNING ] missing heavyatom:  OXT on residue SER:CtermProteinFull 727\n",
      "core.conformation.Conformation: [ WARNING ] missing heavyatom:  OXT on residue SER:CtermProteinFull 844\n",
      "core.conformation.Conformation: Found disulfide between residues 288 299\n",
      "core.conformation.Conformation: current variant for 288 CYS\n",
      "core.conformation.Conformation: current variant for 299 CYS\n",
      "core.conformation.Conformation: current variant for 288 CYD\n",
      "core.conformation.Conformation: current variant for 299 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 345 357\n",
      "core.conformation.Conformation: current variant for 345 CYS\n",
      "core.conformation.Conformation: current variant for 357 CYS\n",
      "core.conformation.Conformation: current variant for 345 CYD\n",
      "core.conformation.Conformation: current variant for 357 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 407 410\n",
      "core.conformation.Conformation: current variant for 407 CYS\n",
      "core.conformation.Conformation: current variant for 410 CYS\n",
      "core.conformation.Conformation: current variant for 407 CYD\n",
      "core.conformation.Conformation: current variant for 410 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 417 435\n",
      "core.conformation.Conformation: current variant for 417 CYS\n",
      "core.conformation.Conformation: current variant for 435 CYS\n",
      "core.conformation.Conformation: current variant for 417 CYD\n",
      "core.conformation.Conformation: current variant for 435 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 612 725\n",
      "core.conformation.Conformation: current variant for 612 CYS\n",
      "core.conformation.Conformation: current variant for 725 CYS\n",
      "core.conformation.Conformation: current variant for 612 CYD\n",
      "core.conformation.Conformation: current variant for 725 CYD\n",
      "core.conformation.Conformation: Found disulfide between residues 749 823\n",
      "core.conformation.Conformation: current variant for 749 CYS\n",
      "core.conformation.Conformation: current variant for 823 CYS\n",
      "core.conformation.Conformation: current variant for 749 CYD\n",
      "core.conformation.Conformation: current variant for 823 CYD\n",
      "basic.io.database: Database file opened: sampling/antibodies/cluster_center_dihedrals.txt\n",
      "protocols.antibody.AntibodyNumberingParser: Antibody numbering scheme definitions read successfully\n",
      "protocols.antibody.AntibodyNumberingParser: Antibody CDR definition read successfully\n",
      "antibody.AntibodyInfo: Successfully finished the CDR definition\n",
      "antibody.AntibodyInfo:  Could not setup Vl Vh Packing angle for camelid antibody\n",
      "antibody.AntibodyInfo: AC Detecting Camelid CDR H3 Stem Type\n",
      "antibody.AntibodyInfo: AC Finished Detecting Camelid CDR H3 Stem Type: NEUTRAL\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H1\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 13 Omega: TTTTTTTTTTTTT\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H2\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 10 Omega: TTTTTTTTTT\n",
      "antibody.AntibodyInfo: Setting up CDR Cluster for H3\n",
      "protocols.antibody.cluster.CDRClusterMatcher: Length: 10 Omega: TTTTTTTTTT\n"
     ]
    }
   ],
   "source": [
    "parser = get_parser()\n",
    "params = parser.parse_args([])\n",
    "log = get_new_log_dir(os.path.join(params.out_root), date=True)\n",
    "params.log_dir = log\n",
    "dataset = get_sabdab(params)\n",
    "test_samples = []\n",
    "\n",
    "encoder, decoder, dico, bert = get_model(params)\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "samples = dict()\n",
    "identity = {'CDR1':0, 'CDR2':0, 'CDR3':0}\n",
    "i = 0\n",
    "sample = SabdabEntry(dataset=dataset, index=i, params=params)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa5d5643-fc31-4ee1-8706-9c11ce685de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ag_tensor, ag_length_tensor, ag_langs_tensor = build_batch(torch.LongTensor([dico.index(w) for w in sample.antigen]), \n",
    "                                                           params.tgt_id, params.eos_index, params.bos_index, params.pad_index)\n",
    "ab_tensor, ab_length_tensor, ab_langs_tensor = build_batch(torch.LongTensor([dico.index(w) for w in sample.antibody]), \n",
    "                                                           params.tgt_id, params.eos_index, params.bos_index, params.pad_index)\n",
    "token_type_ids = torch.zeros_like(ag_tensor)\n",
    "\n",
    "ab_tensor, ab_length_tensor, ab_langs_tensor, ag_tensor, ag_length_tensor, ag_langs_tensor, token_type_ids = \\\n",
    "    to_cuda(ab_tensor, ab_length_tensor, ab_langs_tensor, ag_tensor, ag_length_tensor, ag_langs_tensor, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8669be2-0c2a-4452-abda-53dbc16a72a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):  \n",
    "    bert_embed = bert(input_ids=ag_tensor.T, token_type_ids=token_type_ids.T, attention_mask=get_masks(ag_tensor.size()[0], ag_length_tensor, False)[0]).last_hidden_state\n",
    "\n",
    "    enc1 = encoder('fwd', x=ag_tensor, lengths=ag_length_tensor, langs=ag_langs_tensor, causal=False, bert_embed=bert_embed)\n",
    "    enc1 = enc1.transpose(0, 1)\n",
    "ab_weights_tensor, _, _ = build_batch(torch.LongTensor(sample.weights.get('CDR3', sample.weights['CDR123'])), params.tgt_id, 0, 0, 0)\n",
    "ab_weights_tensor,  = to_cuda(ab_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b11ab62a-6955-4bd3-8450-238616f0cccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Optimize(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, bert, sample, params, dico):\n",
    "        super(Optimize, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.bert_word_embedding = bert.embeddings.word_embeddings\n",
    "        self.bert = bert\n",
    "        ag_tensor, ag_length_tensor, ag_langs_tensor = build_batch(torch.LongTensor([dico.index(w) for w in sample.antigen]), \n",
    "                                                           params.tgt_id, params.eos_index, params.bos_index, params.pad_index)\n",
    "        ab_tensor, ab_length_tensor, ab_langs_tensor = build_batch(torch.LongTensor([dico.index(w) for w in sample.antibody]), \n",
    "                                                           params.tgt_id, params.eos_index, params.bos_index, params.pad_index)\n",
    "        ab_weights_tensor, _, _ = build_batch(torch.LongTensor(sample.weights.get('CDR3', sample.weights['CDR123'])), params.tgt_id, 0, 0, 0)\n",
    "        self.ab_weights_tensor,  = to_cuda(ab_weights_tensor)\n",
    "\n",
    "        self.lr = 100\n",
    "\n",
    "        self.ab_tensor, self.ab_length_tensor, self.ab_langs_tensor, self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor= \\\n",
    "        to_cuda(ab_tensor, ab_length_tensor, ab_langs_tensor, ag_tensor, ag_length_tensor, ag_langs_tensor)\n",
    "        \n",
    "        \n",
    "    def encode(self, input_ids, length_tensor, langs, one_hot=None):\n",
    "        token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_ids = token_type_ids.cuda()\n",
    "        if one_hot is None:\n",
    "            one_hot = self.one_hot(input_ids)\n",
    "        bert_embed_x = self.one_hot_to_embed(one_hot, self.bert_word_embedding)\n",
    "        encoder_embed_x = self.one_hot_to_embed(one_hot, self.encoder.embeddings)\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):  \n",
    "            bert_embed = self.bert(inputs_embeds=bert_embed_x, token_type_ids=token_type_ids.T, attention_mask=get_masks(input_ids.size()[0], length_tensor, False)[0]).last_hidden_state\n",
    "\n",
    "            enc1 = self.encoder('fwd', x=input_ids, lengths=length_tensor, langs=langs, causal=False, bert_embed=bert_embed, embed_x=encoder_embed_x)\n",
    "            enc1 = enc1.transpose(0, 1)\n",
    "        return enc1, bert_embed\n",
    "      \n",
    "        \n",
    "    def decode(self, input_ids, length_tensor, langs, enc, enc_len, bert_embed, one_hot=None):\n",
    "        alen = torch.arange(length_tensor.max(), dtype=torch.long, device=input_ids.device)\n",
    "        pred_mask = (alen[:, None] < length_tensor[None] - 1)[:-1]   # do not predict anything given the last target word\n",
    "        pred_mask = pred_mask.expand([input_ids.shape[0]-1,1])\n",
    "        y = input_ids[1:].masked_select(pred_mask)\n",
    "        if one_hot is None:\n",
    "            one_hot = self.one_hot(input_ids)\n",
    "        embed_x = self.one_hot_to_embed(one_hot, self.decoder.embeddings)\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):\n",
    "            dec = self.decoder('fwd', x=input_ids, lengths=length_tensor, causal=True, langs=langs,\n",
    "                          src_enc=enc, src_len=enc_len, bert_embed=bert_embed,\n",
    "                          embed_x=embed_x\n",
    "                        )\n",
    "            scores, loss = decoder('predict', tensor=dec, pred_mask=pred_mask, y=y, get_scores=True)\n",
    "            scores = F.log_softmax(scores, dim=-1).exp()\n",
    "            loss = -scores[torch.arange(input_ids.shape[0]-1),y].mean()\n",
    "\n",
    "\n",
    "        return loss, scores\n",
    "\n",
    "    def one_hot(self, tensor, noise=False):\n",
    "        one_hot_tensor = F.one_hot(tensor, num_classes=30).type(torch.float)\n",
    "        if noise:\n",
    "            one_hot_tensor = self.normal_label_smoothing(one_hot_tensor, self.ab_weights_tensor)\n",
    "        return one_hot_tensor\n",
    "    \n",
    "    def one_hot_to_embed(self, tensor, module):\n",
    "        return tensor.matmul(module.weight).transpose(0,1)\n",
    "\n",
    "    def normal_noise(self, one_hot, weight, epsilon=10, mean=0.0, std_dev=100):\n",
    "        normal_distribution = torch.normal(mean, std_dev, one_hot.shape)\n",
    "        normal_distribution = torch.abs(normal_distribution)\n",
    "        normal_distribution = normal_distribution / normal_distribution.sum(2).unsqueeze(2).expand(normal_distribution.shape)  # Normalize the distribution to sum to 1\n",
    "        new_tensor = epsilon * normal_distribution.cuda()\n",
    "        result = one_hot.clone()\n",
    "\n",
    "        result[(weight == 1).flatten()] = new_tensor[(weight == 1).flatten()]\n",
    "        # result[(weight == 1).flatten(),0, :5] = 0\n",
    "        return result\n",
    "    \n",
    "    def normal_label_smoothing(self, one_hot, weight, epsilon=0.2, mean=0.0, std_dev=1):\n",
    "        normal_distribution = torch.normal(mean, std_dev, one_hot.shape)\n",
    "        normal_distribution = torch.abs(normal_distribution)\n",
    "        normal_distribution = normal_distribution / normal_distribution.sum(2).unsqueeze(2).expand(normal_distribution.shape)  # Normalize the distribution to sum to 1\n",
    "        new_tensor = (1 - epsilon) * one_hot + epsilon * normal_distribution.cuda()\n",
    "        result = one_hot.clone()\n",
    "\n",
    "        result[(weight == 1).flatten()] = new_tensor[(weight == 1).flatten()]\n",
    "        return result\n",
    "        \n",
    "    def one_hot_ab(self):\n",
    "        return self.normal_label_smoothing(self.one_hot(self.ab_tensor), self.ab_weights_tensor).detach().type(torch.float).requires_grad_()\n",
    "\n",
    "    def forward(self, ab_onehot):\n",
    "        temp = ab_onehot.clone()\n",
    "        temp[(self.ab_weights_tensor == 1).flatten(),0,:5] = 0\n",
    "\n",
    "        enc, bert = self.encode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor)\n",
    "        loss, scores = self.decode(temp.argmax(-1), self.ab_length_tensor, self.ab_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert, F.softmax(ab_onehot, -1))\n",
    "        loss = torch.matmul(scores, F.softmax(ab_onehot, -1).squeeze(1).T+1e-5)[torch.arange(118), torch.arange(118)].mean()\n",
    "        print(loss.item())\n",
    "\n",
    "        Frobenius_loss = torch.norm(F.softmax(ab_onehot, -1) - self.one_hot(self.ab_tensor), dim=2).mean()\n",
    "        \n",
    "        print(Frobenius_loss)\n",
    "        l1_norms = torch.norm(F.softmax(ab_onehot, -1), p=1, dim=-1)\n",
    "        reg_term = torch.abs(l1_norms)\n",
    "        print(reg_term.mean())\n",
    "        loss = loss #+ 0.05*reg_term.mean()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        changed, new_ab_onehot = self.update(ab_onehot)\n",
    "        print(changed)\n",
    "        with torch.no_grad():\n",
    "            print('new onehot ab-ag')\n",
    "            enc, bert = self.encode(self.ab_tensor, self.ab_length_tensor, self.ab_langs_tensor, new_ab_onehot)\n",
    "            loss, scores = self.decode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "            print(loss.item())\n",
    "\n",
    "            print('new ag-ab')\n",
    "            temp = new_ab_onehot.clone()\n",
    "            temp[(self.ab_weights_tensor == 1).flatten(),0,:5] = 0\n",
    "\n",
    "            enc, bert = self.encode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor)\n",
    "            loss, scores = self.decode(temp.argmax(-1), self.ab_length_tensor, self.ab_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "            print(loss.item())\n",
    "\n",
    "            print('original ab-ag')\n",
    "            enc, bert = self.encode(self.ab_tensor, self.ab_length_tensor, self.ab_langs_tensor)\n",
    "            loss, scores = self.decode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "            print(loss.item())\n",
    "\n",
    "\n",
    "            print('original ag-ab')\n",
    "            enc, bert = self.encode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor)\n",
    "            loss, scores = self.decode(self.ab_tensor, self.ab_length_tensor, self.ab_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "\n",
    "            print(loss.item())\n",
    "            \n",
    "        # new_ab_onehot[(self.ab_weights_tensor == 1).flatten()] = (new_ab_onehot/new_ab_onehot.sum(-1).unsqueeze(2))[(self.ab_weights_tensor == 1).flatten()]\n",
    "        # new_ab_onehot[new_ab_onehot < 0] = 0\n",
    "        return new_ab_onehot\n",
    "\n",
    "    def update(self, ab_onehot):\n",
    "        new_ab_onehot = ab_onehot - \\\n",
    "            ((ab_onehot.grad * self.lr) * self.ab_weights_tensor.unsqueeze(2))\n",
    "        changed = not (self.ab_tensor == new_ab_onehot.argmax(-1)).all()\n",
    "        # new_ab_onehot[(self.ab_weights_tensor == 1).flatten(),0,:5] = 0\n",
    "        # self.smoothed_weight = self.sum_one(new_ab_onehot).detach().requires_grad_()\n",
    "        return changed, new_ab_onehot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f301106e-6fec-4ec0-92c0-02bac5547d80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[0.0614],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[20],\n",
       "        [13],\n",
       "        [14],\n",
       "        [ 8],\n",
       "        [19],\n",
       "        [19],\n",
       "        [14],\n",
       "        [ 8]], device='cuda:0'))"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = Optimize(encoder, decoder, bert, sample, params, dico)\n",
    "o.ab_weights_tensor[100:] = 0\n",
    "ab_onehot = o.normal_noise(o.one_hot_ab(), o.ab_weights_tensor).detach().type(torch.float)\n",
    "ab_onehot[(o.ab_weights_tensor == 0).flatten()] = ab_onehot[(o.ab_weights_tensor == 0).flatten()] * 100\n",
    "ab_onehot = ab_onehot.requires_grad_()\n",
    "# o.ab_tensor[101] = 10\n",
    "F.softmax(ab_onehot[99:107],-1).max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "d19e4f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0775437131524086\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "True\n",
      "new onehot ab-ag\n",
      "-0.05587519332766533\n",
      "new ag-ab\n",
      "-0.9163196682929993\n",
      "original ab-ag\n",
      "-0.05872029438614845\n",
      "original ag-ab\n",
      "-0.9474751353263855\n"
     ]
    }
   ],
   "source": [
    "ab_onehot = o(ab_onehot.detach().type(torch.float).requires_grad_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "4ee3c8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[0.0657],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[18],\n",
       "        [13],\n",
       "        [14],\n",
       "        [ 8],\n",
       "        [19],\n",
       "        [19],\n",
       "        [14],\n",
       "        [ 8]], device='cuda:0'))"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(ab_onehot[99:107],-1).max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "c392cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1769,  0.5026,  0.5507,  0.1713,  0.8702,  0.3840,  0.4714,  0.0934,\n",
      "         -0.0408,  0.0294, -0.0264,  0.1005,  0.1775, -0.0373,  0.0919,  0.2429,\n",
      "          0.3876, -0.0404,  0.9800,  0.0295,  0.8210,  0.3156,  0.2815,  0.4407,\n",
      "         -0.0431,  0.5557,  0.9140,  0.3180,  0.9323,  0.3493]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0604, 0.0234], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ab_onehot[99])\n",
    "F.softmax(ab_onehot,-1)[99,0,18:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "75d34215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(128.8144, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norms = torch.norm(ab_onehot, p=1, dim=2)\n",
    "reg_term = torch.sum(torch.abs(l1_norms - 1))\n",
    "reg_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "0d2e48db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 99, 101], device='cuda:0'),)\n",
      "tensor([[-1.3193, -0.7160, -0.5079, -0.0474, -1.2667,  0.0269,  0.1399,  0.1745,\n",
      "          6.9504,  3.3534,  1.0780,  2.7917,  0.1616,  0.3097, -0.0782, -0.1693,\n",
      "          1.2231, -0.2720,  0.6096, -0.5265, -0.2374, -0.6325,  0.5630,  0.5040,\n",
      "          0.7064,  0.1829, -1.3335, -0.5932,  0.1758, -1.2510]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(8, device='cuda:0')\n",
      "tensor([19], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.logical_not((o.ab_tensor == ab_onehot.argmax(-1)).flatten()).nonzero(as_tuple=True))\n",
    "print(ab_onehot[99])\n",
    "print(ab_onehot[99].argmax())\n",
    "print(o.ab_tensor[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "bbe5703f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[CLS]', 'E', 'F', 'Q', 'L', 'Q', 'Q', 'S', 'G', 'P', 'E', 'L',\n",
       "       'V', 'K', 'P', 'G', 'A', 'S', 'V', 'K', 'I', 'S', 'C', 'K', 'A',\n",
       "       'S', 'G', 'Y', 'S', 'F', 'T', 'D', 'Y', 'N', 'I', 'N', 'W', 'M',\n",
       "       'K', 'Q', 'S', 'N', 'G', 'K', 'S', 'L', 'E', 'W', 'I', 'G', 'V',\n",
       "       'V', 'I', 'P', 'K', 'Y', 'G', 'T', 'T', 'N', 'Y', 'N', 'Q', 'K',\n",
       "       'F', 'Q', 'G', 'K', 'A', 'T', 'L', 'T', 'V', 'D', 'Q', 'S', 'S',\n",
       "       'S', 'T', 'A', 'Y', 'I', 'Q', 'L', 'N', 'S', 'L', 'T', 'S', 'E',\n",
       "       'D', 'S', 'A', 'V', 'Y', 'Y', 'C', 'T', 'R', 'F', 'R', 'D', 'V',\n",
       "       'F', 'F', 'D', 'V', 'W', 'G', 'T', 'G', 'T', 'T', 'V', 'T', 'V',\n",
       "       'S', 'S', '[SEP]'], dtype='<U5')"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([dico.id2word[s.item()] for s in o.ab_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "4325f37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[CLS]', 'E', 'F', 'Q', 'L', 'Q', 'Q', 'S', 'G', 'P', 'E', 'L',\n",
       "       'V', 'K', 'P', 'G', 'A', 'S', 'V', 'K', 'I', 'S', 'C', 'K', 'A',\n",
       "       'S', 'G', 'Y', 'S', 'F', 'T', 'D', 'Y', 'N', 'I', 'N', 'W', 'M',\n",
       "       'K', 'Q', 'S', 'N', 'G', 'K', 'S', 'L', 'E', 'W', 'I', 'G', 'V',\n",
       "       'V', 'I', 'P', 'K', 'Y', 'G', 'T', 'T', 'N', 'Y', 'N', 'Q', 'K',\n",
       "       'F', 'Q', 'G', 'K', 'A', 'T', 'L', 'T', 'V', 'D', 'Q', 'S', 'S',\n",
       "       'S', 'T', 'A', 'Y', 'I', 'Q', 'L', 'N', 'S', 'L', 'T', 'S', 'E',\n",
       "       'D', 'S', 'A', 'V', 'Y', 'Y', 'C', 'T', 'R', 'R', 'R', 'D', 'V',\n",
       "       'F', 'C', 'I', 'V', 'W', 'G', 'T', 'G', 'T', 'T', 'V', 'T', 'V',\n",
       "       'S', 'S', '[SEP]'], dtype='<U5')"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([dico.id2word[s.item()] for s in ab_onehot.argmax(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "c3d58212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0040, 0.0041, 0.0021, 0.0028, 0.0009, 0.0054, 0.0195, 0.0010, 0.0026,\n",
       "         0.0041, 0.0010, 0.0015, 0.0067, 0.0159, 0.0012, 0.6727, 0.0016, 0.0038,\n",
       "         0.1036, 0.0022, 0.0019, 0.0025, 0.0032, 0.0357, 0.0167, 0.0491, 0.0096,\n",
       "         0.0029, 0.0151, 0.0065]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.gumbel_softmax(ab_onehot[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8af4529-0f5f-4f61-ac17-8416594c826f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'proj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [98], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m pred_mask \u001b[38;5;241m=\u001b[39m (alen[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m<\u001b[39m ab_length_tensor[\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]   \u001b[38;5;66;03m# do not predict anything given the last target word\u001b[39;00m\n\u001b[1;32m     21\u001b[0m y \u001b[38;5;241m=\u001b[39m ab_tensor[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mmasked_select(pred_mask)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mproj\u001b[49m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(decoder\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mweight[\u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m25\u001b[39m, :])\n\u001b[1;32m     24\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m ab_tensor\n\u001b[1;32m     25\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(input_tensor, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'proj' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def update(input_tensor, ab_weights_tensor):\n",
    "    new_tensor = input_tensor.clone().detach()\n",
    "    print('******')\n",
    "    step = 0.1\n",
    "    while (F.one_hot(new_tensor.argmax(2), num_classes=30) == input_tensor).all() and \\\n",
    "        (new_tensor.argmax(2)[new_tensor.argmax(2).flatten() != input_tensor.argmax(2).flatten()] > 4).all() and \\\n",
    "            (new_tensor.argmax(2)[new_tensor.argmax(2).flatten() != input_tensor.argmax(2).flatten()] < 25).all():\n",
    "        new_tensor = new_tensor + ((input_tensor.grad * step) * ab_weights_tensor.transpose(0,1).unsqueeze(2).expand(input_tensor.shape).cuda())\n",
    "        step += 0.1\n",
    "        print(1)\n",
    "    # print((new_tensor.flatten() != input_tensor.flatten()).argmax())\n",
    "    print(input_tensor.argmax(2)[new_tensor.argmax(2).flatten() != input_tensor.argmax(2).flatten()])\n",
    "\n",
    "    print(new_tensor.argmax(2)[new_tensor.argmax(2).flatten() != input_tensor.argmax(2).flatten()])\n",
    "\n",
    "    return new_tensor.argmax(2)\n",
    "        \n",
    "\n",
    "alen = torch.arange(ab_length_tensor.max(), dtype=torch.long, device=ab_length_tensor.device)\n",
    "pred_mask = (alen[:, None] < ab_length_tensor[None] - 1)[:-1]   # do not predict anything given the last target word\n",
    "y = ab_tensor[1:].masked_select(pred_mask)\n",
    "proj.weight = torch.nn.Parameter(decoder.embeddings.weight[5:25, :])\n",
    "\n",
    "input_tensor = ab_tensor\n",
    "input_tensor = F.one_hot(input_tensor, num_classes=30).type(torch.float).detach().requires_grad_()\n",
    "# embed_x = decoder.embeddings.weight[input_tensor,:].transpose(0,1).detach().requires_grad_()\n",
    "\n",
    "for i in range(2):\n",
    "    embed_x = input_tensor.matmul(decoder.embeddings.weight).transpose(0,1)\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True): \n",
    "        dec = decoder('fwd', x=ab_tensor, lengths=ab_length_tensor, langs=ab_tensor.clone().fill_(params.tgt_id), causal=True,\n",
    "                            src_enc=enc1, src_len=ag_length_tensor, bert_embed=bert_embed, embed_x=embed_x\n",
    "                        )\n",
    "        scores, loss = decoder('predict', tensor=dec, pred_mask=pred_mask, y=y, get_scores=True)\n",
    "        s = F.log_softmax(scores, dim=-1)\n",
    "        loss = s[torch.arange(s.shape[0]), input_tensor.argmax(2)[1:].flatten()].sum()\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        input_tensor = update(input_tensor, ab_weights_tensor.transpose(0,1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ce3cc37-29b6-423e-bb88-cc886726735b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([119, 1, 30])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad76ef76-b568-4332-824e-0f51f79a322c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.flatten() == ab_tensor.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d85c6030-5c67-4a8a-8750-1ff32c6dccac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0013,  0.1574, -0.0698,  ..., -0.1786,  0.0265,  0.0289],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((input_tensor.grad * 1e0) * ab_weights_tensor.transpose(0,1).unsqueeze(2).expand(input_tensor.shape).cuda())[0,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32632d1a-e4b5-4692-b4c8-5a4525d617bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = torch.nn.Sequential(\n",
    "    bert.embeddings.position_embeddings,\n",
    "    bert.embeddings.token_type_embeddings,\n",
    "    bert.embeddings.LayerNorm,\n",
    "    bert.embeddings.dropout,\n",
    "    bert.encoder,\n",
    "    bert.pooler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e56144e0-aa92-4143-95ad-707e2a767e28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(40000, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-29): 30 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "af885efc-0faf-4ffb-a553-0d2174213fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_tensor.flatten() == input_tensor.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f2a46c7c-f95c-4fd3-9f5b-bd98fa0154d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad5c31-cc42-4257-a70b-ece6c819ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.slen = ab_tensor.shape[0]\n",
    "#         self.s = 1\n",
    "#         self.ab_tensor = ab_tensor\n",
    "#         self.ab_tensor_batch = self.ab_tensor.expand([self.slen,self.s])\n",
    "#         self.ab_weight = ab_weight.detach()\n",
    "#         self.ab_length = ab_length.detach()\n",
    "#         self.ab_length_batch = self.ab_length.expand([self.s])\n",
    "\n",
    "#         self.src_enc = src_enc.expand([self.s, src_enc.shape[1], 1024]).detach()\n",
    "#         self.src_len = src_len.expand([self.s]).detach()\n",
    "#         self.bert_embed = bert_embed.expand([self.s, src_enc.shape[1], 1024]).detach()\n",
    "#         self.alen = torch.arange(ab_length.max(), dtype=torch.long, device=ab_length.device)\n",
    "#         self.pred_mask = (self.alen[:, None] < ab_length[None] - 1)[:-1]   # do not predict anything given the last target word\n",
    "#         self.pred_mask = self.pred_mask.expand([self.slen-1,self.s])\n",
    "#         self.y = ab_tensor[1:].masked_select(self.pred_mask)\n",
    "#         self.weight = F.one_hot(ab_tensor, num_classes=30).type(torch.float)\n",
    "#         self.weight = self.weight.expand([self.slen,self.s,30])\n",
    "#         self.smoothed_weight = self.normal_label_smoothing(self.weight, self.ab_weight)\n",
    "#         # self.weight = self.weight + (torch.normal(0, .2, self.weight.shape).cuda()* self.ab_weight.unsqueeze(2).expand(self.weight.shape))\n",
    "#         self.weight = self.weight.detach().requires_grad_()\n",
    "#         self.last_weight = self.weight.detach().clone()\n",
    "#         self.smoothed_weight = self.smoothed_weight.detach().requires_grad_()\n",
    "\n",
    "#         self.softmax = torch.nn.Softmax(dim=2)\n",
    "#         self.proj = decoder.embeddings.weight\n",
    "#         self.first = True\n",
    "#         self.lr = 1e-2\n",
    "#         self.loss = 0\n",
    "\n",
    "    def sum_one(self, tensor):\n",
    "        return tensor / tensor.sum(2).unsqueeze(2).expand(tensor.shape)\n",
    "    \n",
    "    def forward(self, decoder):\n",
    "        weight = self.smoothed_weight\n",
    "        \n",
    "        # weight[(optimizer.ab_weight==1)][:,5:25] = self.linear.weight\n",
    "        # weight = self.weight\n",
    "        # loss_sum = -torch.abs((self.weight.sum(dim=2) - 1)).mean(1).mean()\n",
    "        # loss_reg = torch.norm(weight * self.ab_weight.unsqueeze(2).cuda(), p=2, dim=2).mean()\n",
    "\n",
    "        # loss_max = (self.weight.max(dim=2)[0] - 1).mean(1).mean()\n",
    "        loss_reg = -(self.smoothed_weight ** 2).sum(2).mean()\n",
    "        embed = weight.matmul(decoder.embeddings.weight).transpose(0,1)\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):\n",
    "            dec = decoder('fwd', x=self.ab_tensor_batch, lengths=self.ab_length_batch,\n",
    "                          langs=self.ab_tensor_batch.clone().fill_(params.tgt_id), causal=True,\n",
    "                          src_enc=self.src_enc, src_len=self.src_len, bert_embed=self.bert_embed,\n",
    "                          embed_x=embed\n",
    "                        )\n",
    "            scores, loss = decoder('predict', tensor=dec, pred_mask=self.pred_mask, y=self.y, get_scores=True)\n",
    "            scores = F.log_softmax(scores, dim=-1)\n",
    "            loss = scores[torch.arange(118), self.y].sum()\n",
    "            slen = scores.shape[0]\n",
    "            # loss = weight[1:].view(-1,30).matmul(scores.transpose(1,0)).view((slen,slen)).mul(torch.eye(slen).cuda())\n",
    "            # loss = loss[torch.arange((self.slen-1)*self.s), torch.arange((self.slen-1)*self.s)].view([self.s,-1]).mean(1).mean()\n",
    "\n",
    "        # print(scores[torch.arange(scores.shape[0]), optimizer.smoothed_weight[1:].argmax(2).flatten()].sum())    \n",
    "        # return loss\n",
    "        # loss = -F.cross_entropy(scores[-1].unsqueeze(0), self.weight[-1])\n",
    "        print(loss)\n",
    "        # one_hot_target = torch.zeros_like(self.weight).scatter_(1, target_index.unsqueeze(1), 1)\n",
    "        total_loss = loss_reg + loss.mean()\n",
    "        print(loss_reg)\n",
    "\n",
    "        if self.first:\n",
    "            self.loss = loss\n",
    "            self.first = False\n",
    "            \n",
    "        total_loss.backward()\n",
    "        flag = self.update()        \n",
    "        self.new_ab = self.smoothed_weight.argmax(2)\n",
    "        return flag,loss.mean()\n",
    "        \n",
    "    \n",
    "    def update(self):\n",
    "        # print(self.weight.grad[100])\n",
    "        new_weight = self.smoothed_weight + \\\n",
    "            ((self.smoothed_weight.grad * self.lr) * self.ab_weight.unsqueeze(2).expand(self.weight.shape))\n",
    "        changed = (self.ab_tensor == new_weight.argmax(-1)).all()\n",
    "\n",
    "        self.smoothed_weight = self.sum_one(new_weight).detach().requires_grad_()\n",
    "        return changed\n",
    "    \n",
    "    def one_hot(self):\n",
    "        t = self.smoothed_weight.clone()\n",
    "        t[2:-2, 0, :5] = 0\n",
    "        new_weight = F.one_hot(t.argmax(dim=2), num_classes=30).type(torch.float).detach().requires_grad_()        \n",
    "        self.smoothed_weight = self.normal_label_smoothing(new_weight, self.ab_weight).detach().requires_grad_()\n",
    "        self.y = self.new_ab[1:].masked_select(self.pred_mask)\n",
    "\n",
    "\n",
    "# ab_test = ab_tensor.clone()\n",
    "# ab_test[100] = 5\n",
    "# optimizer = Optimize(decoder, ab_test, ab_weights_tensor, ab_length_tensor, enc1, ag_length_tensor, bert_embed)\n",
    "# opt = torch.optim.Adam(optimizer.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b04efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    def forward_reverse(self, ab_onehot):\n",
    "        enc, bert = self.encode(self.ab_tensor, self.ab_length_tensor, self.ab_langs_tensor, ab_onehot)\n",
    "        loss = self.decode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "        print(loss.item())\n",
    "        \n",
    "        kl_loss = F.kl_div((ab_onehot+1e-3).log(), (self.one_hot(self.ab_tensor)+1e-5).type(torch.float), reduction='batchmean')\n",
    "\n",
    "        # kl_loss = F.kl_div(ab_onehot, self.one_hot(self.ab_tensor), reduction='batchmean')\n",
    "        print(kl_loss)\n",
    "        l1_norms = torch.norm(ab_onehot, p=1, dim=2)\n",
    "        reg_term = torch.abs(l1_norms - 1)\n",
    "        # print(reg_term[90:110])\n",
    "        loss = loss + 0.005*kl_loss #+ 0.5*reg_term.sum()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        changed, new_ab_onehot = self.update(ab_onehot)\n",
    "        print(changed)\n",
    "        with torch.no_grad():\n",
    "            print('new onehot ab-ag')\n",
    "            enc, bert = self.encode(self.ab_tensor, self.ab_length_tensor, self.ab_langs_tensor, new_ab_onehot)\n",
    "            loss = self.decode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "            print(loss.item())\n",
    "\n",
    "            print('new ag-ab')\n",
    "            temp = new_ab_onehot.clone()\n",
    "            temp[(self.ab_weights_tensor == 1).flatten(),0,:5] = 0\n",
    "\n",
    "            enc, bert = self.encode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor)\n",
    "            loss = self.decode(temp.argmax(-1), self.ab_length_tensor, self.ab_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "            print(loss.item())\n",
    "\n",
    "            print('original ab-ag')\n",
    "            enc, bert = self.encode(self.ab_tensor, self.ab_length_tensor, self.ab_langs_tensor)\n",
    "            loss = self.decode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "            print(loss.item())\n",
    "\n",
    "\n",
    "            print('original ag-ab')\n",
    "            enc, bert = self.encode(self.ag_tensor, self.ag_length_tensor, self.ag_langs_tensor)\n",
    "            loss = self.decode(self.ab_tensor, self.ab_length_tensor, self.ab_langs_tensor, enc, torch.tensor([enc.shape[1]]).cuda(), bert)\n",
    "\n",
    "            print(loss.item())\n",
    "            \n",
    "        new_ab_onehot[(self.ab_weights_tensor == 1).flatten()] = (new_ab_onehot/new_ab_onehot.sum(-1).unsqueeze(2))[(self.ab_weights_tensor == 1).flatten()]\n",
    "        new_ab_onehot[new_ab_onehot < 0] = 0\n",
    "        return new_ab_onehot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AntiXLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
